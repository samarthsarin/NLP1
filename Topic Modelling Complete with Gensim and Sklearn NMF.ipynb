{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samarth\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from string import punctuation\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel,CoherenceModel\n",
    "from sklearn.decomposition import NMF,LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/Samarth/Desktop/Mtech AI/NLP/Course/NLP_COURSE_FILES/05-Topic-Modeling/npr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article\n",
       "0  In the Washington of 2016, even when the polic...\n",
       "1    Donald Trump has used Twitter  —   his prefe...\n",
       "2    Donald Trump is unabashedly praising Russian...\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...\n",
       "4  From photography, illustration and video, to d..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(set(stopwords.words('english')))+list(punctuation)+['’',\"”\",'—']\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    text = text.lower()\n",
    "    words =  word_tokenize(text)\n",
    "    words = [w for w in words if len(w)>3]\n",
    "    words_stop = [w for w in words if w not in stop_words]\n",
    "    words_lem = [lem.lemmatize(w,'v') for w in words_stop]\n",
    "    return ' '.join(words_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['New']  = df['Article'].apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_gensim(text):\n",
    "    text = text.lower()\n",
    "    words =  word_tokenize(text)    \n",
    "    words = [w for w in words if len(w)>3]    \n",
    "    words_stop = [w for w in words if w not in stop_words]\n",
    "    words_lem = [lem.lemmatize(w,'v') for w in words_stop]\n",
    "    return words_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(df['Article'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_doc = [cleaning_gensim(doc) for doc in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['New']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_df=0.9,min_df=2,stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tfidf.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11992, 43357)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=7,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=7, random_state=42, shuffle=False, solver='cd', tol=0.0001,\n",
       "  verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf.fit(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "want time really make music know people think like say\n",
      "1\n",
      "russia republican obama white house donald say campaign president trump\n",
      "2\n",
      "drug patients people plan coverage medicaid say insurance care health\n",
      "3\n",
      "poll delegate democratic hillary state campaign voters vote sanders clinton\n",
      "4\n",
      "people force city kill isis officer attack report say police\n",
      "5\n",
      "children college parent kid say teachers student education students school\n",
      "6\n",
      "house state vote rule judge president justice supreme senate court\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(nmf.components_):\n",
    "    print(i)\n",
    "    print(' '.join([tfidf.get_feature_names()[k] for k in index.argsort()[-10:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(clean_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in clean_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = LdaModel(corpus=corpus,id2word=dictionary,num_topics=7,random_state=42,passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.028*\"trump\" + 0.013*\"say\" + 0.011*\"clinton\" + 0.010*\"state\" + 0.009*\"president\" + 0.008*\"vote\" + 0.008*\"campaign\" + 0.007*\"would\" + 0.006*\"obama\" + 0.005*\"republican\"'), (1, '0.019*\"say\" + 0.009*\"court\" + 0.008*\"company\" + 0.006*\"report\" + 0.005*\"trump\" + 0.005*\"president\" + 0.005*\"case\" + 0.005*\"would\" + 0.005*\"federal\" + 0.005*\"department\"'), (2, '0.020*\"say\" + 0.011*\"people\" + 0.011*\"like\" + 0.010*\"think\" + 0.009*\"know\" + 0.008*\"go\" + 0.007*\"make\" + 0.007*\"want\" + 0.007*\"time\" + 0.006*\"school\"'), (3, '0.026*\"say\" + 0.008*\"food\" + 0.007*\"water\" + 0.006*\"people\" + 0.005*\"make\" + 0.005*\"like\" + 0.004*\"years\" + 0.003*\"city\" + 0.003*\"come\" + 0.003*\"go\"'), (4, '0.021*\"say\" + 0.012*\"health\" + 0.009*\"people\" + 0.007*\"study\" + 0.007*\"percent\" + 0.006*\"care\" + 0.005*\"would\" + 0.005*\"find\" + 0.005*\"state\" + 0.005*\"also\"'), (5, '0.007*\"like\" + 0.006*\"make\" + 0.005*\"music\" + 0.005*\"time\" + 0.004*\"first\" + 0.004*\"write\" + 0.004*\"book\" + 0.003*\"also\" + 0.003*\"world\" + 0.003*\"show\"'), (6, '0.021*\"say\" + 0.009*\"report\" + 0.008*\"police\" + 0.006*\"people\" + 0.005*\"attack\" + 0.005*\"state\" + 0.005*\"kill\" + 0.004*\"tell\" + 0.004*\"government\" + 0.004*\"country\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.47595543), (1, 0.38612434), (6, 0.1370497)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.011518571)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.get_term_topics('clinton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.319401912610767\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.log_perplexity(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence = CoherenceModel(ldamodel,texts=clean_doc,dictionary=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37205780214972534\n"
     ]
    }
   ],
   "source": [
    "print(coherence.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
